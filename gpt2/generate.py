import argparse

import torch
from torch import Tensor

from tokenizers import Tokenizer

import gpt2.utils as utils
from gpt2.model import GPT, GPTConfig


def parse_opts(parser):
    parser.add_argument(
        '--model',
        help='Path to the model checkpoint',
        required=True,
        type=str,
    )
    parser.add_argument(
        '--tokenizer',
        help='Path to the tokenizer',
        required=True,
        type=str,
    )
    parser.add_argument(
        '--seed',
        help='Seed',
        type=int,
        default=0x3f3f3f3f,
    )
    parser.add_argument(
        '--temperature',
        help='Temperature controls the creativity or randomness of the generated text',
        type=float,
        default=1.0,
    )
    parser.add_argument(
        '--max-new-tokens',
        help='The number of new tokens will be generated by the model',
        type=int,
        default=100,
    )
    parser.add_argument(
        '--top-k',
        help='Take only top-k highest probability tokens',
        type=int,
        default=0,
    )
    parser.add_argument(
        '--top-p',
        help='Take highest probability tokens until the cumulative probability exceeds this value (nucleus sampling)',
        type=float,
        default=1.0,
    )

def main():
    parser = argparse.ArgumentParser(
        description='An example for generating with GPT-2',
    )
    parse_opts(parser)
    args = parser.parse_args()

    utils.set_seed(args.seed)

    device = 'cuda' if torch.cuda.is_available() else 'cpu'
    device = torch.device(device)

    checkpoint_state_dict = torch.load(args.model, map_location=device)
    gpt_config = GPTConfig(**checkpoint_state_dict['config'])
    model = GPT(gpt_config, device=device)
    model.load_state_dict(checkpoint_state_dict['model'])
    model.to(device)

    tokenizer: Tokenizer = Tokenizer.from_file(args.tokenizer)

    while True:
        start_sentence = input('>> ')
        start_ids = tokenizer.encode(start_sentence).ids
        start_ids = Tensor(start_ids).type(torch.int32).unsqueeze_(0).to(device)

        gen_ids = model.generate(
            start_ids,
            max_new_tokens=args.max_new_tokens,
            temperature=args.temperature,
            top_k=args.top_k,
            top_p=args.top_p,
        )
        gen_ids = gen_ids.detach().cpu().numpy()[0]
        gen_sentence = tokenizer.decode(gen_ids, skip_special_tokens=False)
        gen_sentence = gen_sentence.replace(' <|endoftext|> ', '\n')
        print(gen_sentence)


if __name__ == '__main__':
    main()
