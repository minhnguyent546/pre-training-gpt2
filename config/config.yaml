# general options
checkpoints_dir: checkpoints
tokenizer: tokenizer.json
seed: 1061109567

# datastet
train_data: oscar-vi/train.dat # path to the pre-processed data files
validation_data: oscar-vi/validation.dat
num_workers: 1 # will be used in load_dataset, DataLoader, etc

# model
seq_length: 128
d_model: 768
num_layers: 12
num_heads: 12
d_ff: 3072
dropout: 0.1
activation: gelu
tie_weights: True

# logging with wandb
wandb:
  logging: true
  project: pre-training-gpt2
  name: base
  resume_id: null
  notes: null
  tags: null

# training
ddp_backend: nccl # nccl, gloo, etc
compile: true # whether to compile the model (for faster training)
matmul_precision: highest # see: https://pytorch.org/docs/2.3/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision
optim:
  type: adamw # possible values are: adam, adamw
  lr: 0.5
  betas:
    - 0.9
    - 0.999
  eps: 1.0e-8
  weight_decay: 1.0e-4
warmup_steps: 4_000
train_batch_size: 32
eval_batch_size: 16
gradient_accum_step: 1
mixed_precision: false # false, fp16, bf16
train_steps: 40_000
valid_steps: 2_000
valid_interval: 3_000
save_interval: 4_000
saved_checkpoint_limit: 10
max_grad_norm: 1.0
preload_checkpoint: null
