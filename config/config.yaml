# general options
checkpoints_dir: checkpoints
seed: 1061109567

# datastet
train_dir: fineweb_edu/train
valid_dir: fineweb_edu/valid
drop_last: false

# model
vocab_size: 50304 # a bit larger than the default GPT-2 vocab size (50257) but much nicer
seq_length: 512
d_model: 768
num_layers: 12
num_heads: 12
d_ff: 3072
dropout: 0.0
activation: gelu
tie_weights: True

# logging with wandb
wandb:
  logging: true
  project: pre-training-gpt2
  name: base
  logging_interval: 500 # time between syncing to wandb
  resume_id: null
  notes: null
  tags: null

# training
ddp_backend: nccl # nccl, gloo, etc
compile: false # whether to compile the model (for faster training)
matmul_precision: highest # see: https://pytorch.org/docs/2.3/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision
optim:
  type: adamw # possible values are: adam, adamw
  lr: 6.0e-4 # lr should be large when using noam decay (e.g. 0.5)
  betas:
    - 0.9
    - 0.999
  eps: 1.0e-8
  weight_decay: 1.0e-4
scheduler:
  decay_method: cosine # cosine, noam
  warmup_steps: 4_000
  min_lr: 6.0e-5 # used in cosine decay
  decay_steps: 40_000 # used in cosine decay
train_batch_size: 32
eval_batch_size: 16
gradient_accum_step: 1
mixed_precision: false # false, fp16, bf16
train_steps: 40_000
valid_steps: 2_000
valid_interval: 3_000
save_interval: 4_000
saved_checkpoint_limit: 10
max_grad_norm: 1.0
from_checkpoint: null

# training on xla device
xla_one_core: false
mp_start_method: fork
ddp: false
use_syncfree_optim: true # use sync-free optimizer for AMP
